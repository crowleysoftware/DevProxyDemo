Similate a completion from Open AI. This example is using Ollama as the local language model client. When a request goes to https://api.openai.com/v1/completions, DevProxy will intercept it and forward it on to Ollama. Ollama will respond and your application will not know that the response didn't actually come from Open AI. This has the benefit of not incurring any cost and it also tests with real completions as opposed to static mocks.